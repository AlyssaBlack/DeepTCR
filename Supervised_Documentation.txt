Help on module DeepTCR_S:

NAME
    DeepTCR_S

CLASSES
    builtins.object
        DeepTCR_S
    
    class DeepTCR_S(builtins.object)
     |  Methods defined here:
     |  
     |  AUC_Curve(self, show_all=True, filename=None)
     |      AUC Curve for both Single Sequence and Whole File Models
     |      
     |      Inputs
     |      ---------------------------------------
     |      show_all: bool
     |          In the case there is only two classes, the method defaults
     |          to producing an curve for only one class. If one desires
     |          to see curves for all classes, set to True.
     |      
     |      filename: str
     |          Filename to save tif file of AUC curve.
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Condition_Kernels(self, kernel=3, sample_batch_size=50, motif_batch_size=1000, top_kernels=10, weight_by_freq=False, sample=None)
     |      DEPRECATED
     |      --------------------------------------
     |      Condition Kernels with differentially used motifs.
     |      
     |      This method determines in a brute-force heuristic which k-mers are differentially used in each cohort prior to
     |      training the classifier. This is particularly useful in the case for low-frequency motifs that may be difficult to
     |      find with gradient descent.
     |      
     |      Inputs
     |      ---------------------------------------
     |      kernel: int
     |          Size of convolutional kernel.
     |      
     |      sample_batch_size: int
     |          Size of sample batch to pass through network for inference.
     |      
     |      motif_batch_size: int
     |          Size of motif batch to pass through the network for inference.
     |      
     |      top_kernels: int
     |          Number of kernels to select per class to condition.
     |      
     |      weight_by_freq: bool
     |          Whether to use the frequency to determine enriched motifs.
     |      
     |      sample: int
     |          Number of motifs to sample to query for differential usage. For computationally more tractable results with
     |          high length kernels, it is best to sample some number of motifs to query.
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Get_Data_SS(self, directory, Load_Prev_Data=False, classes=None, save_data=True, type_of_data_cut='Fraction_Response', data_cut=1.0, n_jobs=40, aa_column_alpha=None, aa_column_beta=None, count_column=None, sep='\t', aggregate_by_aa=True)
     |      Get Data for Single Sequence Classification.
     |      
     |      Parse Data into appropriate inputs for neural network.
     |      
     |      Inputs
     |      ---------------------------------------
     |      directory: str
     |          Path to directory with folders with tsv files are present
     |          for analysis. Folders names become labels for files within them.
     |      
     |      Load_Prev_Data: bool
     |          Loads Previous Data.
     |      
     |      classes: list
     |          Optional selection of input of which sub-directories to use for analysis.
     |      
     |      save_data: bool
     |         Whether to save data to pickle file for later use.
     |      
     |              type_of_data_cut: str
     |          Method by which one wants to sample from the TCRSeq File.
     |      
     |          Options are:
     |              Fraction_Response: A fraction (0 - 1) that samples the top fraction of the file by reads. For example,
     |              if one wants to sample the top 25% of reads, one would use this threshold with a data_cut = 0.25. The idea
     |              of this sampling is akin to sampling a fraction of cells from the file.
     |      
     |              Frequency_Cut: If one wants to select clones above a given frequency threshold, one would use this threshold.
     |              For example, if one wanted to only use clones about 1%, one would enter a data_cut value of 0.01.
     |      
     |              Num_Seq: If one wants to take the top N number of clones, one would use this threshold. For example,
     |              if one wanted to select the top 10 amino acid clones from each file, they would enter a data_cut value of 10.
     |      
     |              Read_Cut: If one wants to take amino acid clones with at least a certain number of reads, one would use
     |              this threshold. For example, if one wanted to only use clones with at least 10 reads,they would enter a data_cut value of 10.
     |      
     |              Read_Sum: IF one wants to take a given number of reads from each file, one would use this threshold. For example,
     |              if one wants to use the sequences comprising the top 100 reads of hte file, they would enter a data_cut value of 100.
     |      
     |      data_cut: float or int
     |          Value  associated with type_of_data_cut parameter.
     |      
     |      n_jobs: int
     |          Number of processes to use for parallelized operations.
     |      
     |      aa_column_alpha: int
     |          Column where alpha chain amino acid data is stored. (0-indexed)
     |      
     |      aa_column_beta: int
     |          Column where beta chain amino acid data is stored.(0-indexed)
     |      
     |      If both column integers are left to None, column with a header containing 'acid' is used as
     |          the amino acid column.
     |      
     |      count_column: int
     |          Column where counts are stored. If set to None, first column with data in integer datatype is used as the
     |          counts column.
     |      
     |      sep: str
     |          Type of delimiter used in file with TCRSeq data.
     |      
     |      aggregate_by_aa: bool
     |          Choose to aggregate sequences by unique amino-acid. Defaults to True. If set to False, will allow duplicates
     |          of the same amino acid sequence given it comes from different nucleotide clones.
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Get_Data_WF(self, directory, Load_Prev_Data=False, classes=None, save_data=True, type_of_data_cut='Fraction_Response', data_cut=1.0, n_jobs=40, aa_column_alpha=None, aa_column_beta=None, count_column=None, sep='\t', aggregate_by_aa=True)
     |      Get Data for Whole Sample Classification.
     |      
     |      Parse Data into appropriate inputs for neural network.
     |      
     |      
     |      Inputs
     |      ---------------------------------------
     |      directory: str
     |          Path to directory with folders with tsv files are present
     |          for analysis. Folders names become labels for files within them.
     |      
     |      Load_Prev_Data: bool
     |          Loads Previous Data.
     |      
     |      classes: list
     |          Optional selection of input of which sub-directories to use for analysis.
     |      
     |      save_data: bool
     |         Whether to save data to pickle file for later use.
     |      
     |      type_of_data_cut: str
     |          Method by which one wants to sample from the TCRSeq File.
     |      
     |          Options are:
     |              Fraction_Response: A fraction (0 - 1) that samples the top fraction of the file by reads. For example,
     |              if one wants to sample the top 25% of reads, one would use this threshold with a data_cut = 0.25. The idea
     |              of this sampling is akin to sampling a fraction of cells from the file.
     |      
     |              Frequency_Cut: If one wants to select clones above a given frequency threshold, one would use this threshold.
     |              For example, if one wanted to only use clones about 1%, one would enter a data_cut value of 0.01.
     |      
     |              Num_Seq: If one wants to take the top N number of clones, one would use this threshold. For example,
     |              if one wanted to select the top 10 amino acid clones from each file, they would enter a data_cut value of 10.
     |      
     |              Read_Cut: If one wants to take amino acid clones with at least a certain number of reads, one would use
     |              this threshold. For example, if one wanted to only use clones with at least 10 reads,they would enter a data_cut value of 10.
     |      
     |              Read_Sum: IF one wants to take a given number of reads from each file, one would use this threshold. For example,
     |              if one wants to use the sequences comprising the top 100 reads of hte file, they would enter a data_cut value of 100.
     |      
     |      data_cut: float or int
     |          Value  associated with type_of_data_cut parameter.
     |      
     |      n_jobs: int
     |          Number of processes to use for parallelized operations.
     |      
     |      aa_column_alpha: int
     |          Column where alpha chain amino acid data is stored. (0-indexed)
     |      
     |      aa_column_beta: int
     |          Column where beta chain amino acid data is stored.(0-indexed)
     |      
     |      If both column integers are left to None, column with a header containing 'acid' is used as
     |          the amino acid column.
     |      
     |      count_column: int
     |          Column where counts are stored. If set to None, first column with data in integer datatype is used as the
     |          counts column.
     |      
     |      sep: str
     |          Type of delimiter used in file with TCRSeq data
     |      
     |      aggregate_by_aa: bool
     |          Choose to aggregate sequences by unique amino-acid. Defaults to True. If set to False, will allow duplicates
     |          of the same amino acid sequence given it comes from different nucleotide clones.
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Get_Train_Valid_Test_SS(self, test_size=0.2)
     |      Train/Valid/Test Splits.
     |      
     |      Divide data for train, valid, test set. Training is used to
     |      train model parameters, validation is used to set early stopping,
     |      and test acts as blackbox independent test set.
     |      
     |      Inputs
     |      ---------------------------------------
     |      test_size: float
     |          Fraction of sample to be used for valid and test set.
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Get_Train_Valid_Test_WF(self, test_size=0.2, LOO=None)
     |      Train/Valid/Test Splits.
     |      
     |      Divide data for train, valid, test set. Training is used to
     |      train model parameters, validation is used to set early stopping,
     |      and test acts as blackbox independent test set. In the case that
     |      Leave-One-Out (LOO) is set to a value, the valid and test sets
     |      have the same data and early stopping is based on the training loss.
     |      
     |      Inputs
     |      ---------------------------------------
     |      test_size: float
     |          Fraction of sample to be used for valid and test set.
     |      
     |      LOO: int
     |          Number of samples to leave-out in Leave-One-Out Cross-Validation
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  K_Fold_CrossVal(self, folds=None, epochs_min=5, batch_size=25, stop_criterion=0.001, kernel=5, units=12, weight_by_class=False, iterations=None, trainable_embedding=True, accuracy_min=None, weight_by_freq=True, plot_loss=False, num_fc_layers=0, units_fc=12, drop_out_rate=0.0, suppress_output=False)
     |      K_Fold Cross-Validation for Whole Sample Classifier
     |      
     |      If the number of samples is small but training the whole sample classifier, one
     |      can use K_Fold Cross Validation to train on all but one before assessing
     |      predictive performance.After this method is run, the AUC_Curve method can be run to
     |      assess the overall performance.
     |      
     |      Inputs
     |      ---------------------------------------
     |      fold_size: int
     |          Number of Folds
     |      
     |      batch_size: int
     |          Size of batch to be used for each training iteration of the net.
     |      
     |      epochs_min: int
     |          Minimum number of epochs for training neural network.
     |      
     |      stop_criterion: float
     |          Minimum percent decrease in determined interval (below) to continue
     |          training. Used as early stopping criterion.
     |      
     |      kernel: int
     |          Size of convolutional kernel.
     |      
     |      units: int
     |          Number of filters to be used for convolutional kernel.
     |      
     |      weight_by_class: bool
     |          Option to weight loss by the inverse of the class frequency. Useful for
     |          unbalanced classes.
     |      
     |      iterations: int
     |          Option to specify how many iterations one wants to complete before
     |          terminating training. Useful for very large datasets.
     |      
     |      trainable_embedding; bool
     |          Toggle to control whether a trainable embedding layer is used or native
     |          one-hot representation for convolutional layers.
     |      
     |      accuracy_min: float
     |          Optional parameter to allow alternative training strategy until minimum
     |          training accuracy is achieved, at which point, training ceases.
     |      
     |      
     |      weight_by_freq: bool
     |          Whether to use frequency to weight each sequence's features.
     |      
     |      plot_loss: bool
     |          To live plot the train/valid/test losses, set to True.
     |      
     |      num_fc_layers: int
     |          Number of fully connected layers following convolutional layer.
     |      
     |      units_fc: int
     |          Number of nodes per fully-connected layers following convolutional layer.
     |      
     |      drop_out_rate: float
     |          drop out rate for fully connected layers
     |      
     |      suppress_output: bool
     |          To suppress command line output with training statisitcs, set to True.
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Monte_Carlo_CrossVal(self, fold=5, test_size=0.25, epochs_min=5, batch_size=25, LOO=None, stop_criterion=0.001, kernel=5, units=12, weight_by_class=False, trainable_embedding=True, accuracy_min=None, weight_by_freq=True, plot_loss=False, num_fc_layers=0, units_fc=12, drop_out_rate=0.0, suppress_output=False)
     |      Monte Carlo Cross-Validation for Whole Sample Classifier
     |      
     |      If the number of samples is small but training the whole sample classifier, one
     |      can use Monte Carlo Cross Validation to train a number of iterations before assessing
     |      predictive performance.After this method is run, the AUC_Curve method can be run to
     |      assess the overall performance.
     |      
     |      Inputs
     |      ---------------------------------------
     |      fold: int
     |          Number of iterations for Cross-Validation
     |      
     |      test_size: float
     |          Fraction of sample to be used for valid and test set.
     |      
     |      LOO: int
     |          Number of samples to leave-out in Leave-One-Out Cross-Validation
     |      
     |      batch_size: int
     |          Size of batch to be used for each training iteration of the net.
     |      
     |      epochs_min: int
     |          Minimum number of epochs for training neural network.
     |      
     |      stop_criterion: float
     |          Minimum percent decrease in determined interval (below) to continue
     |          training. Used as early stopping criterion.
     |      
     |      kernel: int
     |          Size of convolutional kernel.
     |      
     |      units: int
     |          Number of filters to be used for convolutional kernel.
     |      
     |      
     |      weight_by_class: bool
     |          Option to weight loss by the inverse of the class frequency. Useful for
     |          unbalanced classes.
     |      
     |      trainable_embedding; bool
     |          Toggle to control whether a trainable embedding layer is used or native
     |          one-hot representation for convolutional layers.
     |      
     |      accuracy_min: float
     |          Optional parameter to allow alternative training strategy until minimum
     |          training accuracy is achieved, at which point, training ceases.
     |      
     |      
     |      weight_by_freq: bool
     |          Whether to use frequency to weight each sequence's features.
     |      
     |      plot_loss: bool
     |          To live plot the train/valid/test losses, set to True.
     |      
     |      num_fc_layers: int
     |          Number of fully connected layers following convolutional layer.
     |      
     |      units_fc: int
     |          Number of nodes per fully-connected layers following convolutional layer.
     |      
     |      drop_out_rate: float
     |          drop out rate for fully connected layers
     |      
     |      suppress_output: bool
     |          To suppress command line output with training statisitcs, set to True.
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Motif_Identification_SS(self, group, p_val_threshold=0.05)
     |      Motif Identification for Single-Sequence Classifier
     |      
     |      This method looks for enriched features in the predetermined gropu
     |      and returns fasta files in directory to be used with "https://weblogo.berkeley.edu/logo.cgi"
     |      to produce seqlogos.
     |      
     |      Inputs
     |      ---------------------------------------
     |      group: string
     |          Class for analyzing enriched motifs.
     |      
     |      p_val_threshold: float
     |          Significance threshold for enriched features/motifs for
     |          Mann-Whitney UTest.
     |      
     |      Returns
     |      ---------------------------------------
     |      
     |      self.(alpha/beta)_group_features_ss: Pandas Dataframe
     |          Sequences used to determine motifs in fasta files
     |          are stored in this dataframe where column names represent
     |          the feature number.
     |  
     |  Motif_Identification_WF(self, group, p_val_threshold=0.05, cut=95, save_images=False)
     |      Motif Identification for Whole Sample Classifier
     |      
     |      This method looks for enriched features in the predetermined group
     |      and returns fasta files in directory to be used with "https://weblogo.berkeley.edu/logo.cgi"
     |      to produce seqlogos.
     |      
     |      Inputs
     |      ---------------------------------------
     |      group: string
     |          Class for analyzing enriched motifs.
     |      
     |      p_val_threshold: float
     |          Significance threshold for enriched features/motifs for
     |          Mann-Whitney UTest.
     |      
     |      cut: float
     |          Percentile to set threshold for what is considered as a sequence
     |          positive for a feature.
     |      
     |      save_images: bool
     |          In order to save violin plots of feature contribution to each cohort,
     |          set to True.
     |      
     |      Returns
     |      ---------------------------------------
     |      
     |      self.group_features_wf: Pandas Dataframe
     |          Sequences used to determine motifs in fasta files
     |          are stored in this dataframe where column names represent
     |          the feature number.
     |  
     |  One_V_All(self, one_v_all)
     |      One_V_All Binary Classifier
     |      
     |      If one desires to create a binary classifier where they want to compare one cohort against all others, run this method
     |      to divide the data into two cohorts (class of interest vs all else).
     |      
     |      Inputs
     |      ---------------------------------------
     |      one_v_all: str
     |          To create binary classifier between class and all else, specify cohort to compare against all others.
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Train_SS(self, batch_size=1000, epochs_min=10, stop_criterion=0.001, kernel=5, units=12, trainable_embedding=True, weight_by_class=False, num_fc_layers=0, units_fc=12, drop_out_rate=0.0, suppress_output=False)
     |      Train Single-Sequence Classifier
     |      
     |      This method trains the network and saves features values at the
     |      end of training for motif analysis.
     |      
     |      Inputs
     |      ---------------------------------------
     |      batch_size: int
     |          Size of batch to be used for each training iteration of the net.
     |      
     |      epochs_min: int
     |          Minimum number of epochs for training neural network.
     |      
     |      stop_criterion: float
     |          Minimum percent decrease in determined interval (below) to continue
     |          training. Used as early stopping criterion.
     |      
     |      kernel: int
     |          Size of convolutional kernel.
     |      
     |      units: int
     |          Number of filters to be used for convolutional kernel.
     |      
     |      trainable_embedding; bool
     |          Toggle to control whether a trainable embedding layer is used or native
     |          one-hot representation for convolutional layers.
     |      
     |      num_fc_layers: int
     |          Number of fully connected layers following convolutional layer.
     |      
     |      units_fc: int
     |          Number of nodes per fully-connected layers following convolutional layer.
     |      
     |      drop_out_rate: float
     |          drop out rate for fully connected layers
     |      
     |      suppress_output: bool
     |          To suppress command line output with training statisitcs, set to True.
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  Train_WF(self, batch_size=25, epochs_min=10, stop_criterion=0.001, kernel=5, units=12, weight_by_class=False, trainable_embedding=True, accuracy_min=None, weight_by_freq=True, plot_loss=False, num_fc_layers=0, units_fc=12, drop_out_rate=0.0, suppress_output=False)
     |      Train Whole-Sample Classifier
     |      
     |      This method trains the network and saves features values at the
     |      end of training for motif analysis.
     |      
     |      Inputs
     |      ---------------------------------------
     |      batch_size: int
     |          Size of batch to be used for each training iteration of the net.
     |      
     |      epochs_min: int
     |          Minimum number of epochs for training neural network.
     |      
     |      stop_criterion: float
     |          Minimum percent decrease in determined interval (below) to continue
     |          training. Used as early stopping criterion.
     |      
     |      kernel: int
     |          Size of convolutional kernel.
     |      
     |      units: int
     |          Number of filters to be used for convolutional kernel.
     |      
     |      weight_by_class: bool
     |          Option to weight loss by the inverse of the class frequency. Useful for
     |          unbalanced classes.
     |      
     |      trainable_embedding; bool
     |          Toggle to control whether a trainable embedding layer is used or native
     |          one-hot representation for convolutional layers.
     |      
     |      accuracy_min: float
     |          Optional parameter to allow alternative training strategy until minimum
     |          training accuracy is achieved, at which point, training ceases.
     |      
     |      
     |      weight_by_freq: bool
     |          Whether to use frequency to weight each sequence's features.
     |      
     |      plot_loss: bool
     |          To live plot the train/valid/test losses, set to True.
     |      
     |      num_fc_layers: int
     |          Number of fully connected layers following convolutional layer.
     |      
     |      units_fc: int
     |          Number of nodes per fully-connected layers following convolutional layer.
     |      
     |      drop_out_rate: float
     |          drop out rate for fully connected layers
     |      
     |      suppress_output: bool
     |          To suppress command line output with training statisitcs, set to True.
     |      
     |      
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  __init__(self, Name, max_length=40, device='/gpu:0')
     |      Initialize Training Object.
     |      
     |      Initializes object and sets initial parameters.
     |      
     |      Inputs
     |      ---------------------------------------
     |      Name: str
     |          Name of the object.
     |      
     |      max_length: int
     |          maximum length of CDR3 sequence
     |      
     |      Returns
     |      ---------------------------------------
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors defined here:
     |  
     |  __dict__
     |      dictionary for instance variables (if defined)
     |  
     |  __weakref__
     |      list of weak references to the object (if defined)

FILE
    /home/sidhom/DeepTCR_Github/DeepTCR/DeepTCR/DeepTCR_S.py


